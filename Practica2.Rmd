---
title: "Práctica 2"
author: Apredizaje Computacional, Grado en Ingeniería Informática, 2023/2024. Universidad de Murcia. Lola Conde Herrera y Ángel Abellán Pérez
date: "`r Sys.Date()`"
output:
  
  html_document:
    df_print: paged
    highlight: kate
    number_sections: yes
    theme: spacelab
    toc: yes
    toc_float: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introducción

Esta es la segunda práctica y final de la asignatura de Aprendizaje Computacional de 1er cuatrimestre de cuarto curso del Grado en Informática de la Universidad de Murcia para el curso 2023/2024.

# El problema

Se trabaja con un dataset con una entrada para una canción que indica, sobre todo, su nivel de popularidad a diario. El dataset se actualiza diariamente, e incluye, en cada actualización, las 50 canciones de spotify más populares, analizadas en un contexto de 70 paises. El dataset, accesible [aquí](https://aulavirtual.um.es/access/content/group/3891_G_2023_N_N/Laboratorios/spoti.csv.zip), incluye, de esta forma, un total de 3168 canciones, en 105669 entradas diferentes y descritas mediante 25 atributos. El conjunto de atributos es el siguiente

* spotify_id: el identificador de la canción.

* name: el título de la canción.

* artists: el nombre del artista o artistas detrás de la canción.

* daily_rank: el ranking de la canción en el día correspondiente, dentro del top 50.

* daily_movement: el cambio de posición en el ranking, comparado con el día previo.

* weekly_movement: el cambio de posición en el ranking comparado con la semana anterior.

* country: un código correspondiente al país del que se originó la entrada de la canción en el top 50.

* snapshot_date: fecha en la que se recogió el dato correspondiente a la entrada en la tabla.

* is_explicit: si la canción contiene lenguaje explícito o no.

* duration_ms: duración de la canción en milisegundos.

* album_name, album_release: nombre del album, fecha de publicación.

* danceability, energy, key, loudness, mode, speechiness, acousticness, instrumentalness: parámetros sobre el propio sonido de la canción que indican si es bailable, el nivel de energía, la clave en la que está interpretada, el volumen a la que fue grabada en dbs, si la canción está en clave mayor o menor, el nivel de presencia de palabra hablada en la canción, la calidad acústica de la canción, y su predominio de la instrumentación con respecto a lo vocal, respectivamente.

* liveness: si se grabó en directo.

* valence: medida del positivismo transmitido por la canción.

* tempo: el número de beats por minuto de la canción.

* time_signature: nivel en una escala de ritmo musical.

* popularity: medida del nivel de popularidad de la canción, correspondiente a la fecha en la que se generó el registro de entrada.

# Apartado A: Descripción del conjunto de datos

En este apartado se describe brevemente el conjunto de entrenamiento, y se clasifican los predictores en cuatro grupos diferenciando aquellos que no tienen relación con la popularidad de la canción (`popularity`) de aquellos que sí la tienen, y aquellos que son numéricos de los que son categóricos.

Se seleccionan los predictores que no tienen relación con `popularity` de forma clara. Otros predictores podrían no tener relación con `popularity`, pero como no se puede asegurar, se han dejado en el dataset.

## Lectura del dataset

En primer lugar, se almacena el dataset en la variable *datos*:

```{r}
rutaDeDatos <- "spoti.csv"
datos <- read.csv(rutaDeDatos, na.strings = "")
```

Una vez almacenado el dataset que se encontraba en la ruta *rutaDeDatos* en *datos*, se procede describiendo este conjunto de entrenamiento.

## Descripción breve del conjunto de entrenamiento

Se presenta la estructura del dataset. En la primera columna se observa el nombre de cada columna del dataset, en la segunda el tipo de dato que contiene, y en la tercera ejemplos de algunos de los valores que contiene.

```{r}
str(datos)
```

Se puede observar que existen 105669 entradas diferentes, con 24 predictores y la variable a predecir `popularity`. A continuación, se comprueba si en los datos hay NAs.

```{r}
apply(datos, 2, function(y){sum(is.na(y))})
```

Se puede observar NAs en los predictores name, artists, country, album_name, y album_release_date. El tratamiento de los NAs se realizará posteriomente.

Antes de proseguir con los predictores, se procede a comprobar el número de entradas únicas que hay en cada predictor.

```{r}
apply(datos, 2, function(y){length(unique(y))})
```

Se puede observar un total de 29 días distintos en snapshot_date, se va a comprobar el rango de fechas que hay en esta variable para comprobar si son días consecutivos y que días son.

```{r}
min(unique(datos$snapshot_date))
max(unique(datos$snapshot_date))
```
Estos días corresponden a días consecutivos desde mitad de octubre hasta mitad de noviembre.

En spotify_id, hay 3168 valores distintos. Teniendo en cuenta que tenemos 105669 muestras y que el spotify_id se refiere a una canción en concreto, las canciones se repiten mucho en los top 50 de los diferentes paises durante los 29 días. Esto se corresponde a lo visto con anterioridad, ya que es más probable que las canciones sean populares durante un tiempo, a que lo sean un solo día; y al tener 29 días, es más probable que las canciones se repitan en el top 50.

Hay un total de 72 paises diferentes, aunque existen canciones en las que no se puede saber en qué pais se ha tomado la muestra por haber NAs en esta columna.

`popularity` es la variable a predecir, y tiene 101 valores diferentes, que van desde 0 hasta 100 inclusive, como se puede observar a continuación.

```{r}
min(unique(datos$popularity))
max(unique(datos$popularity))
```

## Predictores numéricos que, a priori, no tienen relación alguna con `popularity`

No se ha encontrado ninguno.

## Predictores numéricos que pueden aportar algo a la predicción de `popularity`

### daily_movement y weekly_movement

*daily_movement* indica el cambio de posición en el ranking de la canción con respecto al día anterior. A su vez, *weekly_movement* indica el cambio de posición en el ranking respecto a la semana anterior. Ambos predictores varían en función de las escuchas de una canción en una semana/día, por lo que pueden estar relacionadas con la popularidad, ya que si una canción se escucha más, entonces será cada vez más popular, y su daily_movement y weekly_movement subirá.

### liveness

Dado que *liveness* indica el porcentaje de la canción que se grabó en directo, esto podría influir en la popularidad, ya que algunas personas tienen preferencias sobre cómo las canciones están grabadas.

### loudness

Dado que *loudness* indica el volumen a la que fue grabada la canción en decibelios, esto podría influir en la popularidad de la canción.

### energy

El predictor *energy* indica el nivel de energía de la canción, un aspecto que podría hacer que gustase más o menos a las personas, y por lo tanto influir en su popularidad.

### daily_rank

El top 50 posiciona las canciones en un ranking por paises en función de las reproducciones, la frecuencia con la que se comparte la canción y la cantidad de personas que han descubierto la canción en un periodo determinado. Por esta misma razón, *daily_rank* parece un buen indicador de `popularity`.

### valence

*valance* indica el positivismo transmitido por la canción. Las personas pueden inclinarse por canciones más alegres o bien por más melancólicas, y por lo tanto esto podría influir en su popularidad.

### tempo

Una canción con un tempo muy rápido o muy lento puede no gustar a la mayoría de gente, por lo que *tempo* podría influir en la popularidad de la canción.

### acousticness

El predictor *acousticness* indica la calidad acústica de la canción. Se puede partir de que la calidad de una canción influye en su popularidad.

### instrumentalness

Dado que *instrumentalness* indica el predominio de la instrumentación con respecto a lo vocal, creemos que es un buen predictor para `popularity`, ya que las personas suele tener inclinación por música más o menos intrumental. 

### speechiness

Igual que en el anterior predictor, las personas suelen tener preferencias por canciones más melódicas o más habladas, por lo que  *speechiness*, que indica el nivel de presencia de palabra hablada en la canción, podría influir en la popularidad de la canción.

### danceability

Puede ser un buen indicador de popularidad el hecho de que una canción sea bailable, por lo que este predictor será considerado.

### duration_ms

*duration_ms* es la duración de la canción en milisegundos, un aspecto que se considera para predecir la popularidad ya que una canción muy larga puede no gustar tanto a la gente.

## Predictores categóricos relacionados con `popularity`

### snapshot_date y album_release_date

*snapshot_date* indica la fecha en la que se ha tomado la muestra, y *album_release_date* indica la fecha en la que se ha publicado el album al que pertenece la canción.

La fecha de publicación de un album influye en la popularidad de las canciones que contiene. Esto podría explicarse de una manera más clara con un ejemplo: si un album se publica en 2020, es más probable que las canciones que contiene sean populares en 2020 que en la actualidad (2024), ya que las canciones tienden a escucharse más en el momento en el que se publican o cercano a esta fecha.

Por otro lado, las muestras se pueden haber tomado en cualquier fecha, y no es lo mismo que se haya tomado en 2020 que en 2023. Siguiendo con el ejemplo anterior, no es lo mismo si el album se publicó en 2020 y la muestra se tomó en 2023 que si el album se publicó en 2020 y la muestra se tomó en 2020. Por este motivo, es importante tener en cuenta snapshot_date y album_release_date en conjunto.

Sin embargo, se consiera que estas fechas solo son relevantes en conjunto, ya que por separado no tienen relevancia en la popularidad de las canciones.

Por todo esto, *snapshot_date* y *album_release_date* se consideran buenos predictores de `popularity` en conjunto, pero no en solitario.

### is_explicit

*is_explicit* indica si la canción es explícita o no. Una canción explícita es aquella que contiene lenguaje explícito, es decir, lenguaje que puede resultar ofensivo para algunas personas. Este tipo de canciones tienen restricciones en ciertos paises, y están prohibidas en ciertos espacios públicos o momentos del día. Esto hace que en ciertos países no suelan escuchar canciones explícitas, y por tanto, que no sean populares en ellos.

Por otro lado, por la cultura, en ciertos paises están mal vistas las canciones explícitas, y por tanto, no suelen ser escuchadas.

Además, en ciertas plataformas las canciones explícitas están obligadas a tener silenciadas las partes explícitas, lo que hace que la canción puede no ser tan atractiva para el público.

Por lo tanto, el hecho de que la canción sea explícita podría influir en su popularidad.

### time_signature

Dado que *time_signature* se refiere al compás de la canción y la gran mayoría de la gente está acostumbrada a escuchar canciones en 4/4, podría ser más probable que una canción con este compás sea más popular que una con un compás poco habitual. Por esta razón, time_signature parece un buen predictor para `popularity`.

### mode

*mode* indica si la canción está en modo mayor o menor. Creemos que este aspecto influye en la popularidad de la canción, ya que, al estar en modo mayor, tiene un matiz más alegre que en modo menor, por lo que puede ser más probable que guste a la gente.

### key

*key* se refiere a la tonalidad en la que está interpretada la canción. Las tonalidades mayores suelen sonar alegres y optimistas, mientras que las tonalidades menores pueden transmitir melancolía o tristeza por lo que se cree que podría influir en la popularidad de la canción.

## Predictores categóricos no relacionados con `popularity`

### country

La variable *country* indica el país en el que se ha tomado la muestra.

Esta variable no influye en la popularidad de la canción, ya que la popularidad de una canción no depende del país en el que se escuche. Por esta razón, *country* no se tomará en cuenta para predecir `popularity`.

### name

*name* se refiere al nombre de la canción. En muchos casos, el nombre de una canción es lo primero que se ve de esta, incluso antes de escucharla. El nombre suele ser una palabra o frase con la que el artista identifica su canción. Esto hace que un buen nombre ayude a que un posible oyente clique en la canción para escucharla, lo que da más posibilidades de que le guste y la escuche más a menudo. Además, un nombre corto y conciso, o que resulte "gracioso", es más probable que sea recordado.

Como ejemplo, la siguiente canción: 

```{r}
datos$name[1437]
```
El nombre de esta canción está en un lenguaje no ampliamente conocido en el mundo ni legible en muchos casos, por lo que en el caso de gustarte la canción resulta dificil compartirla para que se expanda su popularidad.

Aun así, esto requeriría realizar un estudio de los nombres y considerar si son o no atractivos, lo que queda fuera del dominio de esta práctica. Por esta razón, no se usará.

### album_name

*album_name* se refiere al nombre del album al que pertenece la canción. Si un album es conocido, es más probable que las canciones sean escuchadas, tanto por fans del artista que quieran escuchar el álbum como por gente que no lo es pero conozcan ese album.

Por otro lado, el nombre del album al que pertenece una canción no es un dato que se suela conocer, y los álbumes no suelen hacerse famosos (las canciones que están en ellos sí). Además, para usar la popularidad del álbum es necesario conocerla, lo que requeriría de un análisis que queda fuera del dominio de la práctica. Por esta razón, el nombre del album no se usará para predecir `popularity`.

Además, la práctica está planteada para evaluar canciones de artistas no conocidos, por lo que su álbum no será conocido. A su vez, el atractivo del nombre de un álbum podría hacer que lo escuchases pero no suele ser común ver los nombres de los álbumes, y calcular el atractivo de los nombres queda fuera del dominio de la práctica.

### artists

*artists* es el nombre del artista o de los artistas a los que pertenece la canción. Existen ciertos artistas que son populares y tienen gran número de fans que siguen su trabajo, lo que hace más probable que sus canciones se hagan populares. A su vez, es más probable que una canción sea escuchada si el artista es conocido, aunque la persona que lo escuche no sea fan suyo.

Si una persona conoce a un artista, es más probable que escuche sus canciones. Aún así, la práctica está planteada para evaluar canciones de cantantes no conocidos, por lo que se descarta el uso de el predictor *artists*. Además, se podría ver el atractivo de los nombres de los artistas, pero realizar esto queda fuera del dominio de la práctica.

### spotify_id

Los valores de esta variable se refieren al identificador de la canción en Spotify. Dado que este dato no es conocido por la gran mayoría del público, y son carácteres que no aportan valor para el publico que los lee, no tiene relación alguna con `popularity`. 

Un ejemplo de esta ristra de caracteres es la siguiente:

```{r}
datos$spotify_id[1]
```

# Apartado B: Estrategias de preparación de los datos

En este apartado se realiza el preprocesamiento de los datos, incluyendo la normalización de los predictores, la creación de nuevos predictores, y la eliminación de predictores que no aportan información.

## Eliminación de *X*

Existe una columna llamada *X*, que se corresponde con el índice de cada entrada. Esta columna es un fallo en la lectura del dataset, por lo que se va a eliminar. 

```{r}
datos <- datos[, -1]
```

## ¿Qué predictores habría que normalizar? ¿Por qué? ¿Cuál sería la estrategia de normalización en cada caso?

Se van a normalizar los predictores numéricos con normalización z, con el uso de scale:

```{r}
datos$daily_rank <- scale(datos$daily_rank)
datos$daily_movement <- scale(datos$daily_movement)
datos$weekly_movement <- scale(datos$weekly_movement)
datos$duration_ms <- scale(datos$duration_ms)
datos$danceability <- scale(datos$danceability)
datos$energy <- scale(datos$energy)
datos$loudness <- scale(datos$loudness)
datos$speechiness <- scale(datos$speechiness)
datos$acousticness <- scale(datos$acousticness)
datos$instrumentalness <- scale(datos$instrumentalness)
datos$liveness <- scale(datos$liveness)
datos$valence <- scale(datos$valence)
datos$tempo <- scale(datos$tempo)
```

Se han normalizado para que los predictores tengan valores en la misma escala. Esto podría ayudar a la comparación de variables que estén en diferentes escalas. Además, algunos algoritmos de aprendizaje automático convergen de manera más rápida si las variables están normalizadas.

Ciertos predictores como duration_ms no tienen un rango de valores definido. En el caso de *duration_ms*, es imposible conocer la duración máxima de cualquier canción. Tal y como se indicó en la parte de teoría de la asignatura, cuando no conocemos el máximo y el mínimo es aconsejable aplicar la normalización z por ser más robusta.

## ¿Podría ser interesante transformar algún atributo o grupos de atributos en uno nuevo? ¿Por qué?

### snapshot_date y album_release_date

Se va a transformar *snapshot_date* y *album_release_date* en una nueva variable llamada *days_since_release*, que indica los días que han pasado desde que se publicó el album al que pertenece la canción hasta que se tomó la muestra. Esto es importante por la razón explicada en anteriores apartados.

Para esto, se va a crear la variable *days_since_release*.

```{r}
datos$days_since_release <- as.numeric(as.Date(datos$snapshot_date) - as.Date(datos$album_release_date))
```

Aunque existen 22 muestras con NAs en *album_release_date*, el resultado de la resta con estas muestras es NA así que no se debe realizar ninguna acción al no tener comportamiento extraño (como se dijo anteriormente, se tratarán los NAs en apartados posteriores).

Se va a normalizar el atributo creado con normalización z, ya que se ha decidido normalizar los atributos numéricos:

```{r}
datos$days_since_release <- scale(datos$days_since_release)
```

### time_signature

Se va a pasar a categórica la variable *time_signature*, ya que se refiere al compás de la canción y no tiene sentido que sea numérica, que le añade un orden falso ya que los distintos valores de la variable no tienen un orden implícito.

```{r}
datos$time_signature <- as.factor(datos$time_signature)
```

### key

Se va a pasar a categórica la variable *key*, ya que se refiere a la tonalidad en la que está interpretada la canción y no tiene sentido que sea numérica.

```{r}
datos$key <- as.factor(datos$key)
```

## ¿Cómo podría aprovecharse el carácter secuencial de los datos?

Esto se aprovechó en la transformación de *snapshot_date* y *album_release_date* en *days_since_release*, ya que se usó la fecha de publicación del album para calcular los días que han pasado desde que se publicó hasta que se tomó la muestra.

A su vez, se va a aprovechar el caracter secuencial creando una nueva variable llamada *daily_movement_popularity*, que indica el cambio de popularity de la canción con respecto al día anterior:

```{r}
library(dplyr)
library(tidyr) # fill

# Se añade un NA, para añadir después el valor de la columna anterior dentro del mismo grupo (o el anterior del anterior hasta llegar al primero que no tenga NA). No se puede hacer en un solo paso.

# Se añade -1000 (valor imposible) en los valores que deberían ser NAs para evitar que se añada el daily_movement_popularity_ayuda anterior. Después, se cambia por NA.
datos <- datos %>%
  arrange(spotify_id, snapshot_date) %>%
  group_by(spotify_id) %>%
  mutate(
    daily_movement_popularity_ayuda = ifelse(
      row_number() == 1,
      -1000, # Si es la primera columna del grupo, se pone -1000 porque no se conoce la popularidad del día anterior
      ifelse(
        snapshot_date == lag(snapshot_date),
        NA, # Si es el mismo día que la columna anterior, se pone NA
        ifelse(
          as.numeric(as.Date(snapshot_date) - as.Date(lag(snapshot_date))) == 1,
          popularity - lag(popularity), # Si la columna anterior es el día anterior, se calcula el valor
          -1000 # Si la columna anterior no es el día anterior, se pone -1000 porque no se conoce la popularidad del día anterior
        )
      )
    )
  ) %>%
  fill(daily_movement_popularity_ayuda) %>% 
  mutate(
    daily_movement_popularity = ifelse(
      daily_movement_popularity_ayuda == -1000,
      NA,
      daily_movement_popularity_ayuda
    )
  ) %>%
  ungroup() %>%
  select(-daily_movement_popularity_ayuda)
```

Tras esto, se va a normalizar el atributo creado con normalización z:

```{r}
datos$daily_movement_popularity <- scale(datos$daily_movement_popularity)
```

## Eliminación de atributos que no tienen relación con `popularity`
En este apartado, se van a eliminar los atributos que no tienen relación con `popularity`, de modo que no se usen en los modelos creados en apartados posteriores.
A su vez, se va a eliminar *snapshot_date* y *album_release_date* debido a que, como se dijo anteriormente, son buenos predictores de `popularity` en conjunto, pero no en solitario; y ya se ha creado el atributo *days_since_release* que los engloba.

```{r}
datos <- datos %>% select(-country, -name, -album_name, -artists, -spotify_id, -snapshot_date, -album_release_date)
```

## Tratamiento de NAs

Debido a que algunos modelos son capaces de trabajar con NAs, se van a dejar los NAs en el dataset. Eliminar las filas con NAs en alguna columna podría hacer que se perdiera información relevante que se encuentra en las otras columnas (predictores) de esas filas.

Si en apartados posteriores se crea un modelo que no acepte NAs, se tratarán en ese momento y para ese modelo en concreto.

# Apartado C: Algoritmos de aprendizaje
## Random forest
### Funcionamiento

Random forest se basa en la creación de un conjunto de árboles de decisión o regresión, dependiendo del problema a resolver. En el caso de predecir *popularity*, al tratarse de una variable numérica, se usarán árboles de regresión.

De este modo, se generan un conjunto de árboles de regresión que se usarán en la predicción final tomando la predicción de cada uno de ellos y combinándola.

Durante la fase de entrenamiento, para generar cada árbol, se toma un nuevo conjunto de entrenamiento. Este conjunto se genera mediante bootstrapping, que consiste en tomar, de forma aleatoria, el mismo número de muestras que el conjunto de entrenamiento original, permitiendo que se repitan.

Una vez se tiene el conjunto de entrenamiento, se genera el árbol correspondiente. En cada nodo, se selecciona de forma aleatoria un subconjunto de predictores, y de ellos se elije el mejor para dividir el nodo.

La parte aleatoria de la creación del modelo permite que este no se sobreajuste a los datos de entrenamiento como ocurría al generar un árbol de decisión o regresión, y por lo tanto, permite una mejor generalización.

Por último, añadir que para predecir una vez creado el modelo, en el caso de decisión, se toma la clase que más árboles han predicho, y en el caso de regresión, se promedian las predicciones de los árboles.

### Requisitos
Random forest (y en específico el paquete 'ranger', que se usará para entrenar al modelo) necesita trabajar con datos sin valores NA, por lo que se va a consultar el número de NAs existentes:

```{r}
apply(datos, 2, function(y){sum(is.na(y))})
```

Como se ha podido observar, existen tan solo 7539 NAs en daily_movement_popularity y 22 en days_since_release. Al tener un dataset con 105669 filas, se consideran muy pocas filas con NAs, por lo que la estrategia seguida será eliminar dichas filas.

```{r}
datos_sin_NAs <- na.omit(datos)
```

### Descripción de hiperparámetros

En primer lugar, se obtiene una tabla con los parámetros que se pueden modificar para crear el modelo:

```{r}
library(caret)
modelInfo = getModelInfo("ranger")
modelInfo = modelInfo$ranger
modelInfo$parameters
```

Como se puede observar, se encuentran 3 parámetros de configuración:

1. *mtry*: Por defecto, es la raíz cuadrada del número de predictores.

2. *splitrule*: Por defecto, se utiliza "variance" para regresión.

3. *min.node.size*: Por defecto, es 5 para regresión (como es el caso de *popularity*).

Como se dijo anteriormente, en el entrenamiento de cada árbol, en cada nodo, se selecciona de forma aleatoria un subconjunto de predictores, y de ellos se elije el mejor para dividir el nodo. Este subconjunto es del tamaño que indica el parámetro *mtry*, y la decisión de qué predictor elegir se hace siguiendo el método indicado en *splitrule*. Por último, *min.node.size* indica el número mínimo de muestras que debe tener un nodo para poder dividirse.

Por último, indicar que la varianza para dividir un nodo en regresión (método por defecto en *splitrule*) se usa de modo que se calcula la varianza de las posibles divisiones. La división que tenga una varianza menor, indica que los valores de la variable a predecir están más concentrados (homogéneos) en los nodos resultantes que en los del resto de divisiones posibles, lo que hace que sea la mejor división (y la que se escoje).

### Grid de hiperparámetros

Debido a que *mtry* suele dar buenos resultados con la raíz cuadrada del número de predictores, se va a probar con valores cercanos a ella. Existen un total de 19 predictores, por lo que su raiz cuadrada es 4.36. Por esta razón, se va a probar con 3, 4, 5, 6, y 7 predictores.

Por otro lado, se va a probar con "variance", "extratrees" y "maxstat" en *splitrule*, y se va a probar con los valores 25, 50, 100, 150, y 200 en *min.node.size*.

Por lo tanto, se crea el grid:

```{r}
mygrid = expand.grid(mtry = c(3,4,5,6,7),
                     splitrule=c("variance","extratrees", "maxstat"),
                     min.node.size  =c(25,50,100,150,200))
```

Una vez creado el grid, se entrenan los modelos. Se va a probar con un conjunto de datos más pequeño, y una vez encontrados los mejores hiperparámetros, se realizará el entrenamiento con el conjunto de datos completo.

Para ello, se crea el conjunto de datos más pequeño con el 10% de las muestras totales, escogidas de forma aleatoria.

```{r}
set.seed(123)
elementos_datos_prueba = sample(1:nrow(datos_sin_NAs), nrow(datos_sin_NAs)*0.1)

datos_prueba = datos_sin_NAs[elementos_datos_prueba,]

inTraining <- createDataPartition(datos_prueba$popularity, times=1,p = .75, list = FALSE)
datos_prueba_train <- datos_prueba[ inTraining,]
datos_prueba_test  <- datos_prueba[-inTraining,]
```

Una vez creado el conjunto de datos de prueba, se entrena el modelo con el grid de hiperparámetros creado. Para ello, se debe incluir el número de árboles del bosque, por lo que se va a probar con 50, 100, 200, 300, y 400 árboles.

```{r}
fitControl <- trainControl(## 10-fold CV
   method = "cv",
   number = 10)
```

```{r eval=FALSE}
ranger_50_trees <- train(popularity ~ ., # Popularity es la variable a predecir, el resto son predictores
   data = datos_prueba_train, # Conjunto de datos de entrenamiento
   method = "ranger",
   trControl = fitControl, 
   tuneGrid=mygrid, 
   num.trees=50,
   importance='impurity')
saveRDS(ranger_50_trees, "ranger_50_trees.rds")
```

Se guardan los resultados en archivos RDS para no tener que entrenar de nuevo los modelos una vez ya entrenados.

```{r}	
ranger_50_trees <- readRDS("ranger_50_trees.rds")
```

```{r}
ranger_50_trees$bestTune
```

Los mejores resultados obtenidos son con un *mtry* de 7, un *min.node.size* de 25, y variance como *splitrule*.

```{r eval=FALSE}
ranger_100_trees <- train(popularity ~ ., # Popularity es la variable a predecir, el resto son predictores
  data = datos_prueba_train, # Conjunto de datos de entrenamiento
  method = "ranger",
  trControl = fitControl, 
  tuneGrid=mygrid, 
  num.trees=100,
  importance='impurity')
saveRDS(ranger_100_trees, "ranger_100_trees.rds")
```

```{r}
ranger_100_trees <- readRDS("ranger_100_trees.rds")
```

```{r}
ranger_100_trees$bestTune
```

Los mejores resultados obtenidos, al igual que con 50 árboles, son con un *mtry* de 7, un *min.node.size* de 25, y variance como *splitrule*.

```{r eval=FALSE}
ranger_200_trees <- train(popularity ~ ., # Popularity es la variable a predecir, el resto son predictores
  data = datos_prueba_train, # Conjunto de datos de entrenamiento
  method = "ranger",
  trControl = fitControl, 
  tuneGrid=mygrid, 
  num.trees=200,
  importance='impurity')
saveRDS(ranger_200_trees, "ranger_200_trees.rds")
```

```{r}
ranger_200_trees <- readRDS("ranger_200_trees.rds")
```

```{r}
ranger_200_trees$bestTune
```

Como se puede observar, los mejores hiperparámetros son los mismos obtenidos anteriormente.

```{r eval=FALSE}
ranger_300_trees <- train(popularity ~ ., # Popularity es la variable a predecir, el resto son predictores
  data = datos_prueba_train, # Conjunto de datos de entrenamiento
  method = "ranger",
  trControl = fitControl, 
  tuneGrid=mygrid, 
  num.trees=300,
  importance='impurity')
saveRDS(ranger_300_trees, "ranger_300_trees.rds")
```

```{r}
ranger_300_trees <- readRDS("ranger_300_trees.rds")
```

```{r}
ranger_300_trees$bestTune
```

Como pasaba con el anterior, los mejores hiperparámetros para 300 árboles son los mismos que se obtuvieron en los anteriores modelos.

```{r eval=FALSE}
ranger_400_trees <- train(popularity ~ ., # Popularity es la variable a predecir, el resto son predictores
  data = datos_prueba_train, # Conjunto de datos de entrenamiento
  method = "ranger",
  trControl = fitControl, 
  tuneGrid=mygrid, 
  num.trees=400,
  importance='impurity')
saveRDS(ranger_400_trees, "ranger_400_trees.rds")
```

```{r}
ranger_400_trees <- readRDS("ranger_400_trees.rds")
``` 

```{r}
ranger_400_trees$bestTune
```

Por último, con 400 árboles, los hiperparámetros que dan los mejores resultados son los mismos que para el resto de número de árboles probados.

Una vez entrenados, se va a observar cómo se comportan los modelos creados con los datos de test (que no se usaron para entrenar el modelo, por lo que nos puede dar una idea de cómo generaliza el modelo). Es importante resaltar que por cada entrenamiento, el modelo creado se ha hecho con los mejores hiperparámetros.

```{r}
modelos <- list()
modelos[[1]] <- ranger_50_trees
modelos[[2]] <- ranger_100_trees
modelos[[3]] <- ranger_200_trees
modelos[[4]] <- ranger_300_trees
modelos[[5]] <- ranger_400_trees

tabla_resultados <- data.frame(RMSE = numeric(),
                               MAE = numeric(),
                               Rsquared = numeric(),
                               numTrees = numeric(),
                               stringsAsFactors = FALSE)

pred <- predict(ranger_50_trees, datos_prueba_test[, -c(4)])
metricas <- postResample(as.numeric(datos_prueba_test[4]$popularity), as.numeric(pred))
tabla_resultados[1, ] <- c(as.numeric(metricas[c("RMSE")]), as.numeric(metricas[c("MAE")]), as.numeric(metricas[c("Rsquared")]), 50)

for (i in 1:4) {
  pred <- predict(modelos[[i+1]], datos_prueba_test[, -c(4)])
  metricas <- postResample(as.numeric(datos_prueba_test[4]$popularity), as.numeric(pred))
  tabla_resultados <- rbind(tabla_resultados, c(as.numeric(metricas[c("RMSE")]), as.numeric(metricas[c("MAE")]), as.numeric(metricas[c("Rsquared")]), i*100))
}

tabla_resultados
```

Como se puede observar, el mejor modelo es el creado con 400 árboles, ya que tiene un mayor *Rsquared* y un menor *MAE* y *RMSE* que el resto de modelos.

Una vez mostrados estos resultados, se va a analizar el grid del conjunto con 400 árboles, ya que es el que mejor resultado ha dado. Se muestran los resultados obtenidos:

```{r}
ranger_400_trees$results
```

Posteriormente, se muestra la gráfica mostrando como cambia *Rsquared* dependiendo de *min.node.size* para cada *splitrule*:

```{r}
library(ggplot2)
library(gridExtra)
library(grid)
library(ggplot2)
library(lattice)
plots <- list()
for (mtry_i in 3:7) {
  filtered_results <- subset(ranger_400_trees$results, mtry == mtry_i)
  plots[[mtry_i - 2]] <- ggplot(data=filtered_results) +
     geom_line(mapping= aes(x=min.node.size, y=Rsquared, group=splitrule, color=splitrule), show.legend = TRUE)
}

grid.arrange(grobs = plots, ncol=2)


```

Como se puede observar, se comporta muy parecida en todos los distintos valores de *mtry*. Para todos ellos, *variance* tiene un mayor valor de *Rsquared*, dismunuyendo progresivamente con el aumento de *min.node.size*. Esto se debe a que cuanto más pequeño sean los nodos finales, mejor predice el modelo para los ejemplos con los que se entrenó.

Como se obtiene un mayor valor con *variance*, y con 25 como *min.node.size*, se observa el cambio de RMSE, MAE y Rsquared para estos, con los distintos valores de *mtry*:

```{r}
filtered_results <- subset(ranger_400_trees$results, (splitrule == "variance" & min.node.size==25))
  
Rsquared <- ggplot(data=filtered_results) +
     geom_line(mapping= aes(x=mtry, y=Rsquared), show.legend = TRUE)

MAE <- ggplot(data=filtered_results) +
     geom_line(mapping= aes(x=mtry, y=MAE), show.legend = TRUE)

RMSE <- ggplot(data=filtered_results) +
     geom_line(mapping= aes(x=mtry, y=RMSE), show.legend = TRUE)
grid.arrange(Rsquared, MAE, RMSE, ncol=3)

```

Como se puede observar, los errores disminuyen y el RSquared aumenta con el aumento de *mtry*, por lo que el modelo tiene mejores resultados cuando *mtry* es 7. 

Se obtiene el mejor modelo con *mtry* 7, *splitrule* variance y *min.node.size* 25,lo que se corresponde con lo observado en las gráficas anteriores.

Ahora, se va a comentar cómo se comporta el modelo creado con los datos de test.

```{r}
pred.test = predict(ranger_400_trees, datos_prueba_test[, -c(4)])
postResample(as.numeric(datos_prueba_test[4]$popularity), as.numeric(pred.test))
```
Se observa que el Rsquared es 0.85, lo que indica que lo que el 85% de la variabilidad de *popularity* se puede explicar con el modelo. El RMSE es de 5.9, y el MAE es de 3.6, valores no muy altos teniendo en cuenta que *popularity* va de 0 a 100.

Por último, se va a comentar cuales son las variables con más importancia en el modelo:

```{r}
varImp <- varImp(ranger_400_trees)
importance <- varImp$importance["Overall"]

names<-row.names(importance)[order(importance$Overall,decreasing = T)]
importance<-importance[order(importance$Overall,decreasing = T),"Overall"]
df_importance = data.frame(Importance = importance, Variable = names)

ggplot(df_importance, aes(x=Variable, y=importance,group=1))+ geom_line(linetype="dashed") + geom_point()+ scale_x_discrete(limits = c(names))+
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))
```

Como se puede observar, existen variables cuyo valor influye mucho a la decisión que toma el modelo, como son la duración de la canción, si es bailable, el tempo, o la energía. A la vez, existen variables que no se consideran relevantes para la construcción del modelo, como son la clave o el movimiento diario. 

Aunque parezca que existen muchas variables sin importancia, se debe resaltar que la mayoría de estas es la misma (key). Por lo tanto, el modelo creado considera importantes muchos de los predictores, aunque no todos.

### Resultados 

A modo de resumen, el modelo random forest con mejores resultados para el problema estudiado, es aquel con los hiperparámetros:

- *splitrule* con valor "variance"
- *min.node.size* con valor 25
- *mtry* con valor 7
- *num.trees* con valor 400

Se va a entrenar el modelo con estos hiperparámetros, usando todo el conjunto de datos en su completitud. En primer lugar, se divide el conjunto de datos entre los datos que se usarán en el entrenamiento, y los que se usarán para el test:

```{r}
inTraining <- createDataPartition(datos_sin_NAs$popularity, times=1,p = .75, list = FALSE)
datos_train <- datos_sin_NAs[ inTraining,]
datos_test  <- datos_sin_NAs[-inTraining,]
```

Una vez obtenido el conjunto de entrenamiento, se entrena el modelo:

```{r eval=FALSE}
mygrid = expand.grid(mtry = 7,
                     splitrule= "variance",
                     min.node.size  = 25)

final_random_forest <- train(popularity ~ .,
   data = datos_train,
   method = "ranger",
   trControl = fitControl, 
   tuneGrid=mygrid, 
   num.trees=400,
   importance='impurity')

saveRDS(final_random_forest, "final_random_forest.rds")
```

```{r}
final_random_forest = readRDS("final_random_forest.rds")
```

Una vez entrenado, se predicen los datos de test, para valorar cómo se comporta el modelo con datos no usados en el entrenamiento, lo que nos da una idea de cómo se comporta el modelo con datos no vistos:

```{r}
pred.test = predict(final_random_forest, datos_test[, -c(4)])
postResample(as.numeric(datos_test[4]$popularity), as.numeric(pred.test))
```

El modelo obtiene un Rsquared de 0.98, lo que indica que es capaz de explicar un 98% de la variablidad de la variable *popularity*. A su vez, los errores obtenidos son 0.64 y 1.9 en MAE y RMSE respectivamente. Estos errores son bajos considerando que la popularidad varía en un rango de 0 a 100, lo que indica que el modelo tiene un buen rendimiento en la predicción de la popularidad.

## Deep learning

En este apartado, se va a entrenar una red neuronal con los datos. Antes de comenzar a explicar qué es una red neuronal, se va a instalar tensorflow con la versión 2.8 para que esto pueda funcionar:

```{r eval=FALSE}
tensorflow::install_tensorflow(version="2.8")
```

Se ha puesto a eval=FALSE, de modo que no se ejecute todas las veces. Si no se tiene instalado tensorflow, se deberá de ejecutar.

### Funcionamiento

Para explicar qué es una red neuronal, en primer lugar, se va a explicar qué es una neurona. Una neurona tiene un conjunto de entradas numéricas. A cada entrada le asocia un peso, de forma que para obtener la salida en cada momento se hace la suma del peso de cada entrada por el valor que venga por cada una. 

Una red neuronal organiza las neuronas en una serie de capas, de forma que todas las neuronas de una capa son entrada a todas las neuronas de la capa siguiente. De esta forma, se tiene una capa de nodos de entrada, un conjunto de capas ocultas, y por último una capa con los nodos de salida (que dan lugar al resultado que da cada vez la red).

Para que la red pueda aprender aspectos complejos (la suma de muchas funciones lineales sigue siendo una función lineal), es necesario que cada una de las neuronas no se vea afectada únicamente por una función lineal para calcular su salida, sino que esta se vea modificada por una función de activación, que modifique esta salida. 

Durante la fase de entrenamiento, un conjunto de datos se utiliza para ajustar los pesos de las conexiones entre las neuronas. Esto se logra mediante el algoritmo de backpropagation, que calcula el error entre la predicción y el valor real, y ajusta los pesos de las conexiones para minimizar este error. Este ajuste de pesos se hace numerosas veces.

De los cuatro algoritmos disponibles en Caret, basados en el paquete Keras (mlpKerasDropout, mlpKerasDecay, mlpKerasDropoutCost y mlpKerasDecayCost), descartaremos mlpKerasDropoutCost y mlpKerasDecayCost, ya que están diseñados para clasificación y no para regresión, como se indica en la documentación de Caret, y nuestro problema es de regresión.

Entre los dos algoritmos restantes, usaremos mlpKerasDecay, ya que se espera que proporcione mejores resultados al distribuir los pesos de manera más uniforme mediante la regularización L2, además de combatir el sobreajuste con dicha regularización.

### Requisitos

Los requisitos para entrenar el modelo son los siguientes:

- Datos sin valores NAs

- Normalización y escalado de datos: Las características con escalas muy diferentes pueden sesgar el modelo y dificultar la convergencia durante el entrenamiento.

- Regularización: Para evitar el sobreajuste, se aplican técnicas de regularización. 

- Tratamiento de variables categóricas: Convertir las variables categóricas en variables numéricas. Los modelos de machine learning requieren datos numéricos para entrenar los modelos, por lo que las única variable categórica que se permite es la variable a predecir (aunque no es el caso en nuestro problema).

En nuestro caso, se usa la regularización L2 ya presente en mlpKerasDecay, por lo que no es necesario hacerla en este apartado. Además, el escalado de datos ya se hizo en un apartado anterior, por lo que tampoco es necesario volverlo a hacer. 

Se va a utilizar la variable ya creada *datos_sin_NAs* que tiene todos los datos sin NAs. Sobre esta variable, se va a realizar el tratamiento de variables categóricas. Para esto se utiliza la función *dummy_cols* de la librería fastDummies, que convierte las variables categóricas (tipo caracter y factor) a numéricas. De este modo, por cada variable se crean tantas variables como valores diferentes haya, y por cada fila, la variable creada que represente el valor que había tendrá 1, y el resto 0.

No se escalan las variables creadas porque estas solo tienen valor 0 o 1.

```{r}
library(fastDummies)
datos_dl = data.frame(datos_sin_NAs) # Se hace una copia
datos_dl = dummy_cols(datos_dl) # Se crean las variables numéricas
datos_dl <- datos_dl %>% select(-key, -is_explicit, -time_signature) #Se eliminan las categóricas (el método no las elimina)

```

### Descripción de hiperparámetros

En primer lugar, se obtiene una tabla con los parámetros que se pueden modificar para crear el modelo:

```{r}
modelInfo = getModelInfo("mlpKerasDecay")
modelInfo = modelInfo$mlpKerasDecay
modelInfo$parameters
```

Como se puede observar, se encuentran 7 parámetros de configuración:

1. *size*: Nº de neuronas en la capa oculta.

2. *lambda*: Parámetro de regularización L2. Cuando se ajustan los pesos en backpropagation, si se utiliza la regularización L2, también se tienen en cuenta los pesos en sí, de forma que se intenta minimizar su valor absoluto, y así evitar sobreaprendizaje. El parámetro lambda indica la importancia del tamaño de los pesos al hacer backpropagation, de modo que cuanto mayor sea, más se tienen en cuenta.

3. *batch_size*: Durante el entrenamiento de la red, se dividen los datos en una serie de conjuntos con el mismo número de muestras (*batch_size*), de modo que se hace backpropagation una vez se pasan todos los elementos de un conjunto. 

4. *lr*: Tasa de aprendizaje. Se utiliza en backpropagation cuando se deben modificar los pesos de las neuronas al tener un mal resultado final. Las neuronas modifican los pesos (sumandoles o restandoles un valor) teniendo en cuenta la equivocación obtenida, y esta modificación se multiplica por el learning rate. De este modo: 

  - Si es muy alto se modifican más los pesos al obtener error: Podría no encontrar un lugar con poco error al dar un salto demasiado grande.
  - Si es muy bajo se modifican en menor medida: Podría quedar atrapado con más facilidad en un punto de mínimo local, donde variando poco los valores se obtienen peor resultado pero con una mayor variación se conseguiría mejor.

5. *rho*: Keras utiliza un optimizador llamado RMSprop para generar el modelo. Este optimizador tiene un parámetro llamado *rho*, que suele dar buenos resultados cuando tiene un valor de 0.9. 

6. *decay*: Durante el entrenamiento de la red neuronal se varía la tasa de aprendizaje del modelo, de forma que a principios del entrenamiento sea mayor, y vaya decayendo conforme pasa el tiempo. Esta reducción se puede controlar con *decay*, de forma que cuanto mayor sea, más rápido decrece la tasa de aprendizaje.

7. *activation*: Qué función de activación se usa en las neuronas de la capa oculta.

### Grid de hiperparámetros

Se va a probar con los siguientes parámetros:

- *size*: Se va a probar con 5, 10 y 19 neuronas en la capa.

- *lambda*: Se va a probar con 0.001, 0.01, 0.1.

- *batch_size*: Se va a probar con 32, 64 y 128.

- *lr*: Se va a probar con 0.001, 0.01 y 0.1.

- *rho*: A *rho* típicamente se le asigna el valor 0.9, por lo que se prueba con este valor.

- *decay*: Se prueban los valores 0.001, 0.01 y 0.02.

- *activation*: Se prueba con las funciones de activación *reLU* y *tanh*.

Con estos valores, se crea el grid:

```{r}

mygridmlp = expand.grid(size =  c(5, 10, 19),
                     lambda = c(0.01, 0.1, 0.001),
                     batch_size = c(32, 64, 128),
                     lr = c(0.1, 0.01, 0.001),
                     rho = c(0.9),
                     decay = c(0.01, 0.02, 0.001),
                     activation = c("tanh", "relu"))
```

Una vez creado el grid, se entrenan los modelos con los distintos hiperparámetros, de forma que se encuentren los que mejor funcionan para nuestros datos. Se va a probar con un conjunto de datos más pequeño (10%):

```{r}
set.seed(123)
elementos_datos_prueba_dl = sample(1:nrow(datos_dl), nrow(datos_dl)*0.1)

datos_prueba_dl = datos_dl[elementos_datos_prueba_dl,]

inTraining_dl <- createDataPartition(datos_prueba_dl$popularity, times=1,p = .75, list = FALSE)
datos_prueba_train_dl <- datos_prueba_dl[ inTraining_dl,]
datos_prueba_test_dl  <- datos_prueba_dl[-inTraining_dl,]
```

Una vez creado el conjunto de datos de prueba, se entrena el modelo con el grid de hiperparámetros creado:

```{r eval=FALSE}
library(caret)
modelos_dl <- caret::train(popularity ~ .,
   data = datos_prueba_train_dl,
   method = 'mlpKerasDecay',
   trControl = trainControl("cv", number=5),
   tuneGrid=mygridmlp
   )

saveRDS(modelos_dl, "modelos_dl.rds")
```

```{r}
modelos_ml <- readRDS("modelos_dl_3.rds")
```

Los resultados de cada modelo son los siguientes:

```{r}
modelos_ml$results
```

A continuación, se muestran los hiperparámetros que mejor resultado han tenido:

```{r}
modelos_ml$bestTune
```

Después, se pasan los datos del test al modelo, para hacernos una idea de cómo de bien funciona este modelo:

```{r eval=FALSE}
datos_prueba_test_dl <- datos_prueba_test_dl %>% select(-popularity)
pred.test = predict(modelos_ml, datos_prueba_test_dl)
```

Y se muestran los resultados:

```{r}
pred.test <- readRDS("predTest_prueba2.rds")
postResample(as.numeric(datos_prueba_test_dl$popularity), as.numeric(pred.test))
```
Como se puede observar, el modelo tiene un Rsquared de 0.511 y un MAE y RMSE de 7.2 y 10.3 respectivamente. No se observan buenos resultados, pero se espera a obtener los resultados al entrenar con todos los datos (dado que al haber más datos, se entrena mejor el modelo).

### Resultados

A modo de resumen, el modelo de redes neuronales con mejores resultados para el problema estudiado, es aquel con los hiperparámetros:

- *size* con valor 19
- *lambda* con valor 0.001
- *batch_size* con valor 64
- *lr* con valor 0.1
- *rho* con valor 0.9
- *decay* con valor 0.001
- *activation* con valor "tanh"

Se va a entrenar el modelo con estos hiperparámetros, usando todo el conjunto de datos en su completitud. En primer lugar, se divide el conjunto de datos entre los datos que se usarán en el entrenamiento, y los que se usarán para el test:

```{r}
inTraining_dl <- createDataPartition(datos_dl$popularity, times=1,p = .75, list = FALSE)
datos_train_dl <- datos_dl[ inTraining,]
datos_test_dl  <- datos_dl[-inTraining,]
```

Una vez obtenidos los datos que se usaran para el entrenamiento, se entrena el modelo con los datos:

```{r eval=FALSE}
mygrid = expand.grid(size = c(19),
                     lambda= c(0.001),
                     batch_size = c(64),
                     lr = c(0.1),
                     rho=c(0.9),
                     decay=c(0.001),
                     activation=c("tanh"))

final_modelo_dl <- train(popularity ~ .,
   data = datos_train_dl,
   method = "mlpKerasDecay",
   trControl = trainControl("cv", number=10), 
   tuneGrid=mygrid)

saveRDS(final_modelo_dl, "final_modelo_dl.rds")
```

```{r}
final_modelo_dl = readRDS("final_modelo_dl2.rds")
```

Una vez entrenado, se predicen los datos de test, para valorar cómo se comporta el modelo con datos no usados en el entrenamiento, lo que nos da una idea de cómo se comporta el modelo con datos no vistos:


```{r eval=FALSE}
datos_test_dl <- datos_test_dl %>% select(-popularity)
pred.test = predict(final_modelo_dl, datos_test_dl)
```

```{r}
pred.test <- readRDS("predTest_final2.rds")
postResample(as.numeric(datos_test_dl$popularity), as.numeric(pred.test))
```

Como se puede observar, se muestra un mejor resultado que en las pruebas con menor número de datos. Sin embargo, las métricas son bastante bajas con respecto a lo obtenido en árboles, por lo que el uso de redes neuronales en este problema no se considera apropiado.

## Tercer modelo

El tercer modelo a crear será el modelo **Support Vector Machines with Radial Basis Function Kernel**, que se denomina 'svmRadial' en la librería 'caret'.

### Funcionamiento

El modelo SVM es un modelo de aprendizaje supervisado que se utiliza tanto para clasificación como para regresión. En el caso de regresión (nuestro problema), busca una función que, dado un dato, devuelva el valor numérico correspondiente.

Para lograrlo, en primer lugar se utilizan los datos de entrenamiento para encontrar la función. Con estos datos:

1. Se transforman los datos a un espacio de dimensiones superiores con un kernel. En el caso de 'svmRadial', se utiliza el kernel RBF.

2. En el espacio de dimensiones superior, se busca el hiperplano que:

  - Contenga el mayor número de puntos dentro de unos márgenes alrededor de este. Es decir, si los puntos están dentro de los márgenes, no se tiene en cuenta la distancia al hiperplano (son correctos).

  - La función que se busca *f(x) = <w, x> + b*, donde *< _ , _ >* es el producto escalar, *w* es un vector y *b* es un escalar. Se busca minimizar *1/2 ||w||^2*, donde ||w|| es la norma de w.

  Para tener en cuenta los puntos que no pueden estar dentro de los márgenes, se introduce el concepto de *slack variables* (*ξ_i*), que mide la distancia de los puntos fuera de los márgenes a los márgenes. De este modo, se busca minimizar *1/2 ||w||^2 + C Σ ξ_i*, donde C es una constante conocida como coste.

  Esta constante *C* sirve para balancear la importancia que se da a minimizar la norma de *w* con respecto a permitir que algunos puntos queden fuera de los márgenes. Si el coste es muy alto, se le dará más importancia a que los puntos estén dentro de los márgenes, y si es muy bajo, se le dará más importancia a minimizar la norma de *w*. El coste es un hiperparámetro del modelo.

3. Una vez se ha encontrado el hiperplano, ya se tiene el modelo. Para predecir, se transforma el dato al espacio de dimensiones superiores, y se aplica la función *f(x)* para obtener el resultado numérico.

### Requisitos

Es necesario que los datos no tengan valores NA, por lo que se necesita tranformar el conjunto de datos en uno sin NAs. Como se ha comentado anteriormente, para nuestro conjunto de datos, se considera apropiado eliminar las filas con NAs, por lo que se trabaja con la variable ya creada *datos_sin_NAs*.

### Descripción de hiperparámetros

En primer lugar, se obtiene una tabla con los parámetros que se pueden modificar para crear el modelo:

```{r}
library(caret)
modelInfo = getModelInfo("svmRadial")
modelInfo = modelInfo$svmRadial
modelInfo$parameters
```

Como se puede observar, se encuentran 2 parámetros de configuración:

1. *C*: Representa el coste, por lo que es un parámetro numérico. 
2, *sigma*: Es un parámetro numérico del kernel (RBF). Es importante, dado que un sigma pequeño puede hace que la función sea más compleja, lo que puede llevar a un sobreajuste. Por otro lado, un sigma grande puede hacer que la función sea más simple, lo que puede llevar a un subajuste.

### Grid de hiperparámetros

Con el fin de probar diversos valores (tanto pequeños como grandes) de cada hiperparámetro, se va a probar con los valores 0.01, 0.1, 1, 10, 100, y 1000 tanto para *C* como para *sigma*.

Se crea el grid:

```{r}
mygrid = expand.grid(C = c(0.01, 0.1, 1, 10, 100, 1000),
                     sigma = c(0.01, 0.1, 1, 10, 100, 1000))
```

Una vez creado el grid, se crean los modelos con los diferentes hiperparámetros. 

Se va a probar con un conjunto de datos más pequeño, y una vez encontrados los mejores hiperparámetros, se realizará el entrenamiento con el conjunto de datos completo. Como ya se creó el conjunto de datos anteriormente (*datos_prueba_train* y *datos_prueba_test*), se va a utilizar el mismo.

```{r eval=FALSE}
fitControl <- trainControl(## 10-fold CV
   method = "cv",
   number = 10)

svm_modelos <- train(popularity ~ ., # Popularity es la variable a predecir, el resto son predictores
   data = datos_prueba_train, # Conjunto de datos de entrenamiento
   method = "svmRadial",
   trControl = fitControl, 
   tuneGrid=mygrid,
   importance='impurity')

saveRDS(svm_modelos, "svm_modelos.rds")
```

Se guardan los resultados en archivos RDS para no tener que entrenar de nuevo los modelos una vez ya entrenados.

```{r}	
svm_modelos <- readRDS("svm_modelos.rds")
```

```{r}
svm_modelos$bestTune
```

El mejor modelo, como se puede observar, es el que se obtiene con sigma con valor 0.1 y C con valor 10.

Posteriormente, se va a observar cómo cambia RSquared en función de C según cada valor de sigma.

```{r}
hiperparametros <- list(0.01, 0.1, 1, 10, 100, 1000)
i = 1
plots <- list()
for (sigma_i in hiperparametros) {
  filtered_results <- subset(svm_modelos$results, sigma == sigma_i)
  plots[[i]] <- ggplot(data=filtered_results) +
     geom_line(mapping= aes(x=C, y=Rsquared), show.legend = TRUE) +
      ggtitle(paste0("sigma = ", as.character(sigma_i)))
  i = i + 1
}

grid.arrange(grobs = plots, ncol=2)
```
Como se puede observar, el valor máximo de Rsquared para cada sigma es distinto, de modo que para sigma 1 y 0.01, se obtiene como máximo un valor aproximado de 0.7. De 1 a 1000, cuanto más grande es el número, más pequeño es el Rsquared máximo que se alcanza. De este modo, el valor máximo se obtiene en sigma con valor 0.1, y cuanto más alejado se encuentre el valor de sigma de 0.1, peor resultados obtiene.

Por otro lado, se observa que todos los valores comienzan con valores de Rsquared muy bajos para C muy baja, pero en cuento aumenta un poco, el valor permanece prácticamente igual. Sin embargo, se observa un "pico" para sigma 0.1 en C = 10.

Esto corresponde con lo observado en "svm_modelos$bestTune", de modo que los mejores valores son con sigma=0.1 y C=10. Ahora, se va a comentar cómo se comporta el modelo creado con los datos de test.

```{r}
pred.test = predict(svm_modelos, datos_prueba_test[, -c(4)])
postResample(as.numeric(datos_prueba_test[4]$popularity), as.numeric(pred.test))
```
Como se puede observar se obtiene un Rsquared de 0.81, indicando que el modelo puede explicar el 81% de la variablidad de la variable *popularity*. A su vez, los errores obtenidos son 3.4 y 6.2 en MAE y RMSE respectivamente (teniendo en cuenta que la popularidad va en un rango de 0 a 100).

Las métricas no son malas, y se prevee (como pasó con los árboles), que con el uso del dataset entero el Rsquared subirá, y MAE y RMSE bajará.

### Resultados

A modo de resumen, el modelo svm con mejores resultados para el problema estudiado, es aquel con los hiperparámetros:

- *C* con valor 10
- *sigma* con valor 0.1

Se va a entrenar el modelo con estos hiperparámetros, usando todo el conjunto de datos en su completitud. Se va a utilizar el conjunto de datos ya dividido entre los datos que se usarán en el entrenamiento (datos_train), y los que se usarán para el test (datos_test).

Se entrena el modelo:

```{r eval=FALSE}
mygrid = expand.grid(C = 10,
                     sigma = 0.1)

final_svm <- train(popularity ~ .,
   data = datos_train,
   method = "svmRadial",
   trControl = fitControl, 
   tuneGrid=mygrid, 
   importance='impurity')

saveRDS(final_svm, "final_svm.rds")
```

```{r}
final_svm = readRDS("final_svm.rds")
```

Una vez entrenado, se predicen los datos de test, para valorar cómo se comporta el modelo con datos no usados en el entrenamiento, lo que nos da una idea de cómo se comporta el modelo con datos no vistos:

```{r}
pred.test = predict(final_svm, datos_test[, -c(4)])
postResample(as.numeric(datos_test[4]$popularity), as.numeric(pred.test))
```

Como se ha podido observar, el modelo tiene un Rsquared de 0.969, por lo que el modelo puede explicar un 96.9% de la variablidad de *popularity*. A su vez, los errores son de 1.2 y 2.54 en MAE y RMSE respectivamente. Por lo tanto, el modelo da buenos resultados para ejemplos no vistos.


# Apartado D: Elección del modelo final

Tal y como indica el enunciado, se va a utilizar el método resamples para observar la diferencia entre los modelos de forma que se pueda escoger el mejor:

```{r}
resamps <- resamples(list(RandomForest=final_random_forest,
                          SVM=final_svm,
                          DeepLearning = final_modelo_dl))

theme1 <- trellis.par.get()
theme1$plot.symbol$col = rgb(.2, .2, .2, .4)
theme1$plot.symbol$pch = 16
theme1$plot.line$col = rgb(1, 0, 0, .7)
theme1$plot.line$lwd <- 2
trellis.par.set(theme1)
bwplot(resamps, layout = c(3, 1))
```

Como se puede observar, la red neuronal (DeepLearning), obtiene un error mucho mayor que los otros dos modelos. A su vez, Rsquared es menor. Como ya se dijo en el apartado correspondiente y se refuerza en el gráfico, la red neuronal no se considera buen modelo.

Entre random forest y SVM, se observa como random forest tiene un Rsquared muy parecido al de SVM (no se ve la diferencia, aunque en los apartados correspondientes obtenían un Rsquared en el test de 0.98 y 0.96 en random forest y SVM respectivamente, siendo levemente mayor el Rsquared del modelo entrenado con random forest).

Sin embargo, tiene un visible menor error (RMSE y MAE). Por lo tanto, el mejor modelo y por lo tanto el modelo final es el entrenado con random forest.